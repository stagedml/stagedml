RuSentiment
===========

In this report we fine-tune multilingual BERT model on a RuSentiment dataset for
Russian language sentiment analysis. We highlight the usage of
[StagedML](https://github.com/stagedml/stagedml) primitives for constucting
models and running experiments.

Resources and related work:

- [RuSentiment project by Text-Machine Lab](http://text-machine.cs.uml.edu/projects/rusentiment/)
  * [Sentiment Annotation Guidelines at GitHub](https://github.com/text-machine-lab/rusentiment)
  * [PwC page](https://paperswithcode.com/paper/rusentiment-an-enriched-sentiment-analysis)
- [Dostoevsky project by Bureaucratic Labs](https://github.com/bureaucratic-labs/dostoevsky)
   * [Dostoevsky project report](https://freesoft.dev/program/132766342)
- [Multilingual BERT by Google Research](https://github.com/google-research/bert/blob/master/multilingual.md)

```{.python .cb.nb}
import altair as alt
import pandas as pd
import numpy as np
from rusentiment_experiment import *
from stagedml.stages.all import *
```

BERT Fine-tuning
----------------

We fine-tuned the following BERT models on a RuSentiment dataset:

1. Multilingual BERT by Google Research, with different learning rates.
2. Ranomly initialized BERT without pre-training.

Multilingual BERT model is defined based on
`all_multibert_finetune_rusentiment` stage of StagedML. We change some default
parameters.

```{.python .cb.code
    include_file=rusentiment_experiment.py
    include_regex="def all_multibert_finetune_rusentiment1.*end1"}
```

Dependency graph of this stage illustrates entities we are going to
calculate in this experiment:

```{.python .cb.nb show=code+stdout:raw+stderr}
with prepare_markdown_image('rusent_graph.png') as path:
  depgraph([all_multibert_finetune_rusentiment1], path)
```

Here,

* `basebert-multi-cased` is a stage which downloads multilingual BERT checkpoint
  published by Google Research.
* `fetchrusent` stage downloads the RuSentiment dataset. Unfortunately, this
  dataset is no longer available in public, so we are using our private copy.
* `tfrecord-rusent` converts the Dataset to the TensorFlow Records format.
* `rusent-pretrained` stage creates the BERT model, loads the pretrained
  checkpoint and performs the fine-tuning on the RuSentiment TFRecords.
* Stages mentioned above are declared in the [StagedML top-level
  collection](https://github.com/stagedml/stagedml/blob/master/src/stagedml/stages/all.py)

We define randomly-initialized BERT by disabling the initialization of the
default version of the above model:

```{.python .cb.code
    include_file=rusentiment_experiment.py
    include_regex="def all_multibert_finetune_rusentiment0.*end0"}
```

We will train models with a number of different learning rates:

```{.python .cb.nb}
print(learning_rates)
```

```{.python .cb.nb}
rref0=realize(instantiate(all_multibert_finetune_rusentiment0))
rrefs=[realize(instantiate(all_multibert_finetune_rusentiment1,lr=lr)) \
                           for lr in learning_rates]
```

For every model trained, we read it's validation logs and plot the accuracy.

```{.python .cb.nb show=code+stdout:raw+stderr}
cols={'steps':[],'accuracy':[],'name':[],'lr':[]}
for rref in [rref0]+rrefs:
  es=tensorboard_tensors(rref,'valid','accuracy')
  assert len(es)>0
  cols['steps'].extend([e.step for e in es])
  cols['accuracy'].extend([te2float(e) for e in es])
  cols['name'].extend([mklens(rref).name.val for _ in es])
  cols['lr'].extend([mklens(rref).lr.val for _ in es])
altair_print(alt.Chart(DataFrame(cols)).mark_line().encode(
  x='steps', y='accuracy', color='lr:N', strokeDash='name'), f'accuracy.png')
```

```{.python .cb.nb show=code+stdout:raw+stderr}
t=BeautifulTable(max_width=1000)
t.set_style(BeautifulTable.STYLE_MARKDOWN)
t.width_exceed_policy=BeautifulTable.WEP_ELLIPSIS
t.column_headers=['Learning rate','Accuracy']
t.numeric_precision=6
for rref in rrefs:
  t.append_row([str(mklens(rref).lr.val),
                te2float(tensorboard_tensors(rref,'eval','eval_accuracy')[-1])])
print(t)
```

Confusion matrix
----------------

We build confusion matrix by (a) defininig a simple model runner and (b) realizing
a stage which uses this runner to calculate the matrix data. Model runner loads
the model referenced by `rref` and process a list of sentences defined by the
user.

```{.python .cb.code
    include_file=rusentiment_experiment.py
    include_regex="class Runner.*runner ends"}
```

Stage for calculating the confusion matrix data is defined as follows:

```{.python .cb.code
    include_file=rusentiment_experiment.py
    include_regex="def bert_rusentiment_evaluation.*eval ends"}
```

We prepare combine primitives to get the final stage to realize in `stage_cm`.

``` {.python .cb.nb}
stage_finetune=partial(all_multibert_finetune_rusentiment1, lr=min(learning_rates))
stage_cm=partial(bert_rusentiment_evaluation, stage=stage_finetune)
```

With the confusion matrix stage added, the dependency graph became:

``` {.python .cb.nb show=code+stdout:raw+stderr}
with prepare_markdown_image('cm_graph.png') as path:
  depgraph([stage_cm], path)
```

Finally, we realize the confusion matrix stage and print the matrix

``` {.python .cb.nb show=code+stdout:raw+stderr}
rref=realize(instantiate(stage_cm))
data:dict={'label':[],'pred':[],'val':[]}
for l,items in readjson(mklens(rref).confusion_matrix.syspath).items():
  for l2,val in items.items():
    data['label'].append(l)
    data['pred'].append(l2)
    data['val'].append(f"{val:.2f}")

base=alt.Chart(DataFrame(data))
r=base.mark_rect().encode(
  y='label:O',x='pred:O', # color='val:Q'
  color=alt.Color('val:Q',
        scale=alt.Scale(scheme='purpleblue'),
        legend=alt.Legend(direction='horizontal')
  ))
t=base.mark_text().encode(y='label:O',x='pred:O',text='val:Q',
    color=alt.condition(
        alt.datum.val < 0.5,
        alt.value('black'),
        alt.value('white')
    )
)
altair_print((r+t).properties(width=300,height=300), 'cm.png')
```

