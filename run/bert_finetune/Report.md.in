BERT fine-tuning experiments
============================

```{.python .cb.nb}
import altair as alt
import pandas as pd
import numpy as np
from bert_finetune_experiment import *
from stagedml.stages.all import *
```


Fine-tuning on GLUE tasks
-------------------------

```{.python .cb.code
    include_file=bert_finetune_experiment.py
    include_regex="experiment_allglue.*return result_allglue"}
```

```{.python .cb.nb show=code}
results=experiment_allglue()
```

Results are:

```{.python .cb.nb show=code+stdout:raw+stderr}
t=BeautifulTable(max_width=1000)
t.set_style(BeautifulTable.STYLE_MARKDOWN)
t.width_exceed_policy = BeautifulTable.WEP_ELLIPSIS
t.column_headers=['Name']+list(results.keys())
t.append_row(['Accuracy, %']+[
  100*protocol_rref_metric(results[tn][0],'evaluate','eval_accuracy')
    for tn in results.keys()])
t.append_row(['F1_score*100']+[
  100*protocol_rref_metric(results[tn][0],'evaluate','f1_score')
    for tn in results.keys()])
t.append_row(['Tr.time, min']+[f"{store_buildelta(rrefs[0])/60:.1f}"
                                     for rrefs in list(results.values())])
print(t)
```

Ref. [Upstream results](https://github.com/google-research/bert#bert)

Batch size in BERT->MRPC fine-tuning
------------------------------------

The top-level procedure of the experiment:

```{.python .cb.code
    include_file=bert_finetune_experiment.py
    include_regex="experiment_bs.*return result_bs"}
```

In the above code we fine-tune BERT-mini model on MRPC task. We increase the
number of epoch to 5 from the default 3, also we want to train `num_instances=5`
models of every batch_size at once (Actually, current implementation trains them
one-by-one, we could parallelize this in future). By using `match_some(5)`
matcher we are saying that we want at least 5 realizations of every
configuration. Now we execute the experiment.

```{.python .cb.nb show=code}
results=experiment_bs(exclude=['+f1v2'])
```

Here is the code to process results. First, we build a collection of
`DataFrame`s.

```{.python .cb.nb}
dfs=[]
for bs,rrefs in results.items():
  for iid,rref in enumerate(rrefs):
    cols={}
    es=tensorboard_tensor_events(rref,'valid','accuracy')
    assert len(es)>0
    cols['steps']=[e.step*bs for e in es]
    cols['valid_accuracy']=[te2float(e) for e in es]
    cols['batch_size']=[bs for _ in es]
    cols['iid']=[iid for _ in es]
    es=tensorboard_tensor_events(rref,'valid','loss')
    assert len(cols['steps'])==len(es)
    cols['valid_loss']=[te2float(e) for e in es]
    dfs.append(DataFrame(cols))
ds=pd.concat(dfs)
```

Finally, we print the validation accuracy of our models. Here:

* `iid` is the instance identifier of the model.
* `batch_size` is the batch size used during fine-tuning
* `steps` is the number of sentences passed through the model. According to the
  value of `max_seq_length` parameter, each sentence contains maximum 128
  tokens.

```{.python .cb.nb show=code+stdout:raw+stderr}
metric='valid_accuracy'
chart=alt.Chart(ds).mark_line().encode(
  x='steps', y=metric, color='batch_size',
  strokeDash='iid')
altair_print(chart, f'figure_{metric}.png')
```

* TODO: find out why do models with smaller batch sizes train better?
  - Is it the effect of batch-normalization?
  - Is it the effect of un-disabled dropout?

Junk
----

```
{.python .cb.nb show=code+stdout:raw+stderr}
metric='train-lr'
dflist=results[metric]
df=pd.concat(dflist)
df=df[(df['batch_size']==32) | (df['batch_size']==2)]
chart=alt.Chart(df).mark_line().encode(
  x='step', y='value', color='batch_size',
  strokeDash='optver')
altair_print(chart, f'figure_{metric}.png')
```

