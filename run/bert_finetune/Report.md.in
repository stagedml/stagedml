BERT fine-tuning experiments
============================

In this report we show various aspects of BERT fine-tuning on GLUE dataset using
[StagedML](https://github.com/stagedml/stagedml) domain-specific language. This
document is a literate Python program rendered with the CodeBraid processor.

```{.python .cb.nb}
from bert_finetune_experiment import *
import altair as alt
import pandas as pd
```

* [Top](/run/bert_finetune)
* [Python utilities for this report](../bert_finetune_experiment.py).
* [Source of this report](../Report.md.in).

Contents
--------

1. [References](#references)
2. [Fine-tuning on GLUE tasks](#fine-tuning-on-glue-tasks)
3. [Batch size VS Learning rate in MNLI-m task](#batch-size-vs-learning-rate-in-mnli-m-task)
4. [Batch size in BERT->MRPC fine-tuning](#batch-size-in-bert-mrpc-fine-tuning)


References
----------

* [Reddit discussion about batch size and learning rate](https://www.reddit.com/r/MachineLearning/comments/84waz4/d_relation_between_learning_rate_batch_size_and/)
* Smith et al. [A Bayesian Perspective on Generalization and Stochastic Gradient
  Descent](https://arxiv.org/abs/1710.06451)
* MacKay, [A practical Bayesian framework for backpropagation
  networks](https://authors.library.caltech.edu/13793/)
* Tishby et al. [Consistent inference of probabilities in layered networks: predictions and
  generalizations](https://ieeexplore.ieee.org/document/118274)

Fine-tuning on GLUE tasks
-------------------------

In this section we fine-tune BERT-mini model on most of GLUE tasks in a loop.
For some tasks, we amend the batch size. We also amend values of `batch_size`
and epoches for some tasks.

### Definitions

```{.python .cb.code
    include_file=bert_finetune_experiment.py
    include_regex="def experiment_allglue.*return result_allglue"}
```

```{.python .cb.nb show=stdout:raw+stderr}
p=join(environ.get('REPIMG',environ['REPOUT']),"graph.png")
stage=partial(all_minibert_finetune_glue,task_name='QQP')
depgraph([stage],filename=p)
print(f"![]({p})")
```

* We loop over all GLUE tasks excluding COLA (TODO: remember why do we exclude
  COLA?)
* For every task, we realize minibert model
  - `realizeMany(instantiate(...))` is the generic procedure of realizing
    Pylightnix stages
  - `redefine(..)` allows us re-define stage's configuration in-place. In our
    case we adjust parameters to match the upstream settings (set `batch_size`,
    4 epoch)
  - Note, that we don't evaluate all possible parameters like the upstream did
    due to time/hardware constraints.
  - `all_minibert_finetune_glue` is defined in `stagedml.stages.all`. By
    realizing it we also realize all it's dependencies, which includes fetching
    the required checkpoints and datasets.
* We build a dictionary containing `RRef` realization reference for every task.
* We also display a graph of stages to be realized in order to realize the
  top-level stage of `all_minibert_finetune_glue(task_name='QQP')`:
* The URL of pre-trained model image:
  `markdown_url(mklens(instantiate(stage).dref).refbert.url.val)`{.python .cb.expr}

### Results

```{.python .cb.nb show=code}
results=experiment_allglue()
```

```{.python .cb.nb show=code+stdout:raw+stderr}
t=BeautifulTable(max_width=1000)
t.set_style(BeautifulTable.STYLE_MARKDOWN)
t.width_exceed_policy = BeautifulTable.WEP_ELLIPSIS
t.column_headers=['Name']+list(results.keys())
t.append_row(['Accuracy, %']+[
  100*protocol_rref_metric(results[tn][0],'evaluate','eval_accuracy')
    for tn in results.keys()])
t.append_row(['F1_score*100']+[
  100*protocol_rref_metric(results[tn][0],'evaluate','f1_score')
    for tn in results.keys()])
t.append_row(['Tr.time, min']+[f"{store_buildelta(rrefs[0])/60:.1f}"
                                  for rrefs in list(results.values())])
print(t)
```

Where:

* `Tr.time` shows training time in seconds, training was done on a single NVidia
  1080Ti GPU.
* See also [reference results by Google
  Research](https://github.com/google-research/bert#bert).

Batch size VS Learning rate in MNLI-m task
------------------------------------------

In this experiment we measure model accuracy for different batch sizes in
learning rates. Learning rate here is the initial learning rate of Polynomial
decay governor, which became active after linear Warmup. The warmup length is
taken to be 10% of the number of training examples exposed to the model.

### Definitions

```{.python .cb.code
    include_file=bert_finetune_experiment.py
    include_regex="def experiment_lr.*return result_lr"}
```

- We train several instances of BERT-mini model on the `DEF_TASK_NAME`{.python
  .cb.expr} task.
- Batch sizes: `DEF_BATCH_SIZES`{.python .cb.expr}
- Peak learning rates: `DEF_LEARNING_RATES`{.python .cb.expr}
- For every `batch size` + `learning rate` combination, we do 4 training
  attempts with different random initialization.

### Results

```{.python .cb.nb show=code}
results=experiment_lr()
```

```{.python .cb.nb show=code+stdout:raw+stderr}
cols={'batch_size':[], 'learning_rate':[], 'eval_accuracy':[], 'attempt':[]}
for (bs,lr),rrefs in results.items():
  for iid,rref in enumerate(rrefs):
    es=tensorboard_tensors(rref,'eval','eval_accuracy')
    cols['eval_accuracy'].extend([te2float(e) for e in es])
    cols['learning_rate'].extend([lr for _ in es])
    cols['batch_size'].extend([bs for _ in es])
    cols['attempt'].extend([iid for _ in es])

df=pd.DataFrame(cols)
chart1=alt.Chart(df).mark_point().encode(
  x='learning_rate',
  y=alt.Y('eval_accuracy', title='Evaluation accuracy, %',
                            scale=alt.Scale(zero=False)),
  shape='batch_size:O',
  color=alt.Color('attempt:O', title='Attempt'))

chart=chart1
for bs in DEF_BATCH_SIZES:
  df2=df[df['batch_size']==bs].drop('attempt',1).groupby('learning_rate',as_index=False).mean()
  chart2=alt.Chart(df2).mark_line().encode(
    x=alt.X('learning_rate', title='Learning rate'),
    y=alt.Y('eval_accuracy', title='Evaluation accuracy, %',
                              scale=alt.Scale(zero=False)),
    shape=alt.Shape('batch_size:O', title='Batch size'))
  chart+=chart2

altair_print(chart, f'figure_eval_accuracy_lr.png')
```

- We see best accuracy on smaller batch sizes with smaller learning rates.
- Increasing batch size may require increasing of learning rate as well.
- Results are qualitatively consistent with the conclusions of [Smith et al.
  paper](https://arxiv.org/abs/1710.06451)
- For `batch_size=64`, best accuracy was achieved with `learning_rate=1e-4`
- For `batch_size=128`, best accuracy was achieved somewhere between
  `learning_rate=1e-4` and `learning_rate=2e-4`

Batch size in BERT->MRPC fine-tuning
------------------------------------

In this section we study how does `batch_size` affect final accuracy of the
model.

### Definitions

```{.python .cb.code
    include_file=bert_finetune_experiment.py
    include_regex="def experiment_bs.*return result_bs"}
```

* We loop over certain batch_sizes and evaluate BERT-mini fine-tuning procedure.
  List of important APIs includes:
  - `realizeMany(instance(..))` runs generice two-pass stage realization
    mechanism of Pylightnix.
  - `redefine(..)` tweaks stage configuration before the realization. Besides
    setting batch_size, we increase number of epoches up to 5.
  - `all_minibert_finetune_glue` is one of StagedML stages, defined in
    `stagedml.stages.all`. By realizing it we also realize all it's
    dependencies, which includes fetching the required images and datasets from
    the Internet.
* `num_instances` parameter of `all_minibert_finetune_glue` stage sets the
  desired number of model instances sharing the same configuration.
* We collect results in a dictionary which maps `batch_sizes` to corresponding
  realization references.

### Results

```{.python .cb.nb show=code}
results=experiment_bs()
```

```{.python .cb.nb show=code+stdout:raw+stderr}
cols={'eval_accuracy':[], 'batch_size':[], 'attempt':[]}
for bs,rrefs in results.items():
  for attempt,rref in enumerate(rrefs):
    es=tensorboard_tensors(rref, 'eval', 'eval_accuracy')
    cols['eval_accuracy'].extend([100.0*te2float(e) for e in es])
    cols['batch_size'].extend([bs for _ in es])
    cols['attempt'].extend([attempt for _ in es])

chartP=alt.Chart(pd.DataFrame(cols)).mark_point().encode(
  x=alt.X('batch_size', title='Batch size'),
  y=alt.Y('eval_accuracy', title='Evaluation accuracy, %',
                           scale=alt.Scale(zero=False)),
  color=alt.Color('attempt:O', title='Attempt'))
chartM=alt.Chart(pd.DataFrame(cols)).mark_line().encode(
  x=alt.X('batch_size', title='Batch size'),
  y=alt.Y('eval_accuracy', title='Evaluation accuracy, %',
                           aggregate='mean',
                           scale=alt.Scale(zero=False)))
altair_print(chartP+chartM, f'figure_eval_accuracy.png')
```

```{.python .cb.nb show=code+stdout:raw+stderr}
cols={'nexamples':[], 'valid_accuracy':[], 'batch_size':[], 'attempt':[]}
for bs,rrefs in results.items():
  for attempt,rref in enumerate(rrefs):
    es=tensorboard_tensors(rref,'valid','accuracy')
    cols['nexamples'].extend([e.step*bs for e in es])
    cols['valid_accuracy'].extend([100.0*te2float(e) for e in es])
    cols['batch_size'].extend([bs for _ in es])
    cols['attempt'].extend([attempt for _ in es])
chart=alt.Chart(pd.DataFrame(cols)).mark_line(point=True).encode(
  x=alt.X('nexamples', title='Training examples seen by the model'),
  y=alt.Y('mean(valid_accuracy)', title='Mean validation accuracy, %',
                                  scale=alt.Scale(zero=False)),
  color=alt.Color('batch_size:O', title='Batch size')
  )
altair_print(chart, f'figure_valid_accuracy.png')
```

* `tensorboard_tensors` is a helper method to access stages TensorBoard
  journals stored in realization's folder.
* `attempt` is the identifier of the training attempt. For any given
  `batch_size`, attempts differ only with the initial values of classification
  head of the model.
* `batch_size` is the batch size used during fine-tuning
* `steps` is the number of sentences passed through the model. According to the
  value of `max_seq_length` parameter, each sentence contains maximum 128
  tokens.
* TODO: find out why do models with smaller batch sizes train better?
  - Is it the effect of batch-normalization (if any)?
  - Is it the effect of dropout not disabled?

