BERT Pre-training
-----------------

This project aims at autmating and reproducing of the following technologies:

1. Generic BERT Pre-training procedure.
2. Experiment described by Zhuohan Li et al. [Train Large, Then Compress:
   Rethinking Model Size for Efficient Training and Inference of
   Transformers](https://arxiv.org/abs/2002.11794).

`Report.md.in` document is a literate Python program to be rendered with the
CodeBraid processor.

See [Report](./out/Report.md)

